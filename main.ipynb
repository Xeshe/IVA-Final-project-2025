{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfce7675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anshe\\Documents\\IVA\\Final Project\\IVA-Final-project-2025\\my_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Anshe\\Documents\\IVA\\Final Project\\IVA-Final-project-2025\\my_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor, \n",
    "    AutoModelForAudioClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emotions                                               Path\n",
      "0    angry      datasets/CREMA-D/AudioWAV/1071_IWL_ANG_XX.wav\n",
      "1    angry  datasets/RAVDESS/Actor_22/03-01-05-02-02-02-22...\n",
      "2    angry      datasets/CREMA-D/AudioWAV/1006_WSI_ANG_XX.wav\n",
      "3    angry      datasets/CREMA-D/AudioWAV/1026_MTI_ANG_XX.wav\n",
      "4    angry      datasets/CREMA-D/AudioWAV/1049_ITS_ANG_XX.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anshe\\AppData\\Local\\Temp\\ipykernel_48556\\950669280.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data_subset = data_path.groupby(\"Emotions\", group_keys=False).apply(lambda x: x.sample(frac=0.3, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = pd.read_csv(\"data_path.csv\")  # Adjust path if needed\n",
    "\n",
    "# Sample 30% of data stratified by emotion\n",
    "data_subset = data_path.groupby(\"Emotions\", group_keys=False).apply(lambda x: x.sample(frac=0.3, random_state=42)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b395d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Emotions                                               Path\n",
      "2661  surprise  datasets/RAVDESS/Actor_07/03-01-08-01-02-02-07...\n",
      "2662  surprise  datasets/RAVDESS/Actor_21/03-01-08-01-02-02-21...\n",
      "2663  surprise  datasets/RAVDESS/Actor_05/03-01-08-02-02-01-05...\n",
      "2664  surprise  datasets/RAVDESS/Actor_04/03-01-08-02-02-02-04...\n",
      "2665  surprise  datasets/RAVDESS/Actor_20/03-01-08-01-02-02-20...\n"
     ]
    }
   ],
   "source": [
    "print(data_subset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a5f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "label2id = {label: i for i, label in enumerate(sorted(data_subset.Emotions.unique()))}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b06a64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model and feature extractor\n",
    "wav2vec2_model_name = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    "extractor = AutoFeatureExtractor.from_pretrained(wav2vec2_model_name)\n",
    "wav2vec2_model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\",\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345b4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, extractor, max_duration=4):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.extractor = extractor\n",
    "        self.max_duration = max_duration\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        path = row['Path']\n",
    "        label = label2id[row['Emotions']]\n",
    "        audio, sr = librosa.load(path, sr=16000)\n",
    "        audio = audio[:int(self.max_duration * sr)]  # Truncate to max duration\n",
    "        inputs = self.extractor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eca0f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(data_subset, test_size=0.1, stratify=data_subset['Emotions'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f40c16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert to Hugging Face-compatible Dataset\n",
    "train_dataset = EmotionDataset(train_df, extractor)\n",
    "val_dataset = EmotionDataset(val_df, extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ac49df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n",
      "('self', 'output_dir', 'overwrite_output_dir', 'do_train', 'do_eval', 'do_predict', 'eval_strategy', 'prediction_loss_only', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'per_gpu_train_batch_size', 'per_gpu_eval_batch_size', 'gradient_accumulation_steps', 'eval_accumulation_steps', 'eval_delay', 'torch_empty_cache_steps', 'learning_rate', 'weight_decay', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'max_grad_norm', 'num_train_epochs', 'max_steps', 'lr_scheduler_type', 'lr_scheduler_kwargs', 'warmup_ratio', 'warmup_steps', 'log_level', 'log_level_replica', 'log_on_each_node', 'logging_dir', 'logging_strategy', 'logging_first_step', 'logging_steps', 'logging_nan_inf_filter', 'save_strategy', 'save_steps', 'save_total_limit', 'save_safetensors', 'save_on_each_node', 'save_only_model', 'restore_callback_states_from_checkpoint', 'no_cuda', 'use_cpu', 'use_mps_device', 'seed', 'data_seed', 'jit_mode_eval', 'use_ipex', 'bf16', 'fp16', 'fp16_opt_level', 'half_precision_backend', 'bf16_full_eval', 'fp16_full_eval', 'tf32', 'local_rank', 'ddp_backend', 'tpu_num_cores', 'tpu_metrics_debug', 'debug', 'dataloader_drop_last', 'eval_steps', 'dataloader_num_workers', 'dataloader_prefetch_factor', 'past_index', 'run_name', 'disable_tqdm', 'remove_unused_columns', 'label_names', 'load_best_model_at_end', 'metric_for_best_model', 'greater_is_better', 'ignore_data_skip', 'fsdp', 'fsdp_min_num_params', 'fsdp_config', 'tp_size', 'fsdp_transformer_layer_cls_to_wrap', 'accelerator_config', 'deepspeed', 'label_smoothing_factor', 'optim', 'optim_args', 'adafactor', 'group_by_length', 'length_column_name', 'report_to', 'ddp_find_unused_parameters', 'ddp_bucket_cap_mb', 'ddp_broadcast_buffers', 'dataloader_pin_memory', 'dataloader_persistent_workers', 'skip_memory_metrics', 'use_legacy_prediction_loop', 'push_to_hub', 'resume_from_checkpoint', 'hub_model_id', 'hub_strategy', 'hub_token', 'hub_private_repo', 'hub_always_push', 'gradient_checkpointing', 'gradient_checkpointing_kwargs', 'include_inputs_for_metrics', 'include_for_metrics', 'eval_do_concat_batches', 'fp16_backend', 'push_to_hub_model_id', 'push_to_hub_organization', 'push_to_hub_token', 'mp_parameters', 'auto_find_batch_size', 'full_determinism', 'torchdynamo', 'ray_scope', 'ddp_timeout', 'torch_compile', 'torch_compile_backend', 'torch_compile_mode', 'include_tokens_per_second', 'include_num_input_tokens_seen', 'neftune_noise_alpha', 'optim_target_modules', 'batch_eval_metrics', 'eval_on_start', 'use_liger_kernel', 'eval_use_gather_object', 'average_tokens_across_devices')\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.TrainingArguments.__init__.__code__.co_varnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2c6acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# TrainingArguments with faster feedback\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_emotion_model\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c06290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anshe\\AppData\\Local\\Temp\\ipykernel_48556\\608578618.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=wav2vec2_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "910fc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from latest checkpoint (optional)\n",
    "latest_checkpoint = \"fine_tuned_emotion_model/checkpoint-1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a13e1623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tlogging_steps: 10 (from args) != 50 (from trainer_state.json)\n",
      "\teval_steps: 50 (from args) != 100 (from trainer_state.json)\n",
      "\tsave_steps: 100 (from args) != 200 (from trainer_state.json)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 3:22:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.726700</td>\n",
       "      <td>1.661416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.796900</td>\n",
       "      <td>1.653152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.667000</td>\n",
       "      <td>1.633661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.633700</td>\n",
       "      <td>1.651510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.661400</td>\n",
       "      <td>1.618276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.620100</td>\n",
       "      <td>1.624880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.687400</td>\n",
       "      <td>1.620500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.656700</td>\n",
       "      <td>1.608483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['fine_tuned_emotion_model\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "\n",
    "# Save the model\n",
    "wav2vec2_model.save_pretrained(\"fine_tuned_emotion_model\")\n",
    "extractor.save_pretrained(\"fine_tuned_emotion_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5a8a539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 01:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.608483076095581,\n",
       " 'eval_runtime': 113.9105,\n",
       " 'eval_samples_per_second': 2.344,\n",
       " 'eval_steps_per_second': 0.588,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be3527",
   "metadata": {},
   "source": [
    "## HELLO START RUNNING EVERYTHING FROM HERE IF YOU JUST WANT THE MAIN LOOP AND NO TRAINING/TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "37c6b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor, \n",
    "    AutoModelForAudioClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "\n",
    "wav2vec2_model = AutoModelForAudioClassification.from_pretrained(\"fine_tuned_emotion_model\")\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"fine_tuned_emotion_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88c58801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def record_audio(filename=\"recorded.wav\", duration=3, fs=16000):\n",
    "    print(\"🎙️ Recording...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()\n",
    "    write(filename, fs, recording)\n",
    "    print(f\"✅ Saved: {filename}\")\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b6b189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_emotion(path):\n",
    "    audio, sr = librosa.load(path, sr=16000)  # model expects 16kHz\n",
    "    inputs = extractor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = wav2vec2_model(**inputs).logits\n",
    "\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    label = wav2vec2_model.config.id2label[pred_id]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a77f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the pipeline once\n",
    "asr = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "def transcribe_wav_clip(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe a .wav file using Hugging Face Wav2Vec2 (offline).\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the .wav file\n",
    "    \n",
    "    Returns:\n",
    "        str: Transcribed text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "\n",
    "        # Convert stereo to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0).unsqueeze(0)\n",
    "\n",
    "        # Resample to 16kHz if necessary\n",
    "        if sr != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "            waveform = resampler(waveform)\n",
    "            sr = 16000\n",
    "\n",
    "        # Prepare input\n",
    "        input_data = {\n",
    "            \"array\": waveform.squeeze().numpy(),\n",
    "            \"sampling_rate\": sr\n",
    "        }\n",
    "\n",
    "        # Run transcription\n",
    "        result = asr(input_data)\n",
    "        return result[\"text\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transcribing '{path}': {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe1cfc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2c44939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai_model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d2cfa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time Gemini setup\n",
    "from logging import config\n",
    "\n",
    "from sklearn import config_context\n",
    "\n",
    "\n",
    "genai_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "chat = genai_model.start_chat(history=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [\"You are an empathetic assistant who chats naturally with users. They are speaking out loud, so their messages might contain typos, filler words, or incomplete thoughts. Please interpret their intent and respond clearly, gently, and naturally.\"]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "def generate_reply_with_context(transcript, emotion):\n",
    "    user_message = f\"[Emotion: {emotion}] {transcript}\"\n",
    "    try:\n",
    "        response = chat.send_message(f\"The user said (may contain errors): {user_message}\")\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Gemini error: {e}\")\n",
    "        return \"[Could not generate response.]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db23b651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤 Voice interaction loop started. Press Ctrl+C to stop.\n",
      "🔴 Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Recording: 100%|██████████| 4/4 [00:04<00:00,  1.00s/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recording saved.\n",
      "🗣️ Transcript: HELLOA GERMAN I\n",
      "😊 Detected Emotion: fear\n",
      "🤖 Gemini: Oh my goodness, it sounds like you're feeling really scared right now.  Is something wrong?  \"Hallo\" is German for hello, so I'm guessing you might be in a situation involving German speakers or Germany itself?  Can you tell me more about what's happening so I can try to help?  Even if it's just a little bit, anything you can share would be useful.\n",
      "\n",
      "🔴 Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Recording: 100%|██████████| 4/4 [00:04<00:00,  1.00s/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recording saved.\n",
      "🗣️ Transcript: NO I AM NOT SCARE GREAT NEW\n",
      "😊 Detected Emotion: disgust\n",
      "🤖 Gemini: Okay, I understand.  You said you're *not* scared, and that's great news!  I apologize if I misinterpreted your initial message.  It sounded like you were distressed, so I wanted to check in.  My mistake!  Is there something else you'd like to talk about, or was there something else that caused that feeling of disgust?  I'm happy to listen if you'd like to share.\n",
      "\n",
      "🔴 Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Recording: 100%|██████████| 4/4 [00:04<00:00,  1.00s/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recording saved.\n",
      "🗣️ Transcript: I AM SAYING YOUR NAME\n",
      "😊 Detected Emotion: disgust\n",
      "🤖 Gemini: Oh, I see.  You're expressing disgust because of my name?  I understand.  Is there something about my name that you find unpleasant?  Or is it something else entirely? I'm programmed to be helpful and harmless, and I want to make sure I'm not causing any offense.  Perhaps we could try to talk about it, if you are comfortable doing so. I’m here to listen.\n",
      "\n",
      "🔴 Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Recording: 100%|██████████| 4/4 [00:04<00:00,  1.00s/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recording saved.\n",
      "🗣️ Transcript: NO I WAS JUST SAYING HALLO\n",
      "😊 Detected Emotion: disgust\n",
      "🤖 Gemini: Ah, I see!  You were just saying \"Hallo\" –  like a test, perhaps?  My apologies for the misinterpretations.  I'm still under development and learning to understand nuances in language and emotion.  I appreciate you clarifying. Is there anything else I can help you with today?\n",
      "\n",
      "🔴 Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Recording: 100%|██████████| 4/4 [00:04<00:00,  1.00s/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recording saved.\n",
      "🗣️ Transcript: ITSOUK\n",
      "😊 Detected Emotion: sad\n",
      "🤖 Gemini: It sounds like you're feeling sad.  \"Itsouk\" isn't a word I recognize, so I'm not entirely sure what you mean. Could you tell me a little more about what's making you feel sad?  There's no pressure, but if you'd like to talk about it, I'm here to listen.\n",
      "\n",
      "🔴 Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Recording: 100%|██████████| 4/4 [00:04<00:00,  1.00s/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recording saved.\n",
      "🗣️ Transcript: I MEAN MAYBE IT JOST SOME DEPRESSED\n",
      "😊 Detected Emotion: fear\n",
      "🤖 Gemini: I understand. You're saying it might just be depression, and that's making you feel afraid.  It's completely understandable to be scared when dealing with feelings of depression.  Depression can be a very difficult thing to manage. Would you like to talk about it?  Perhaps we can explore some resources or ideas that might help. Or, if you prefer, I can simply listen without offering suggestions.  Whatever feels most comfortable for you is fine.\n",
      "\n",
      "🔴 Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Recording: 100%|██████████| 4/4 [00:04<00:00,  1.00s/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recording saved.\n",
      "🗣️ Transcript: \n",
      "😊 Detected Emotion: sad\n",
      "⚠️ No speech detected.\n",
      "\n",
      "🔴 Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Recording: 100%|██████████| 4/4 [00:04<00:00,  1.00s/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recording saved.\n",
      "🗣️ Transcript: \n",
      "😊 Detected Emotion: sad\n",
      "⚠️ No speech detected.\n",
      "\n",
      "🔴 Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Recording: 100%|██████████| 4/4 [00:04<00:00,  1.00s/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recording saved.\n",
      "🗣️ Transcript: I DOREALLY KNOW WHAT ELSE TO SAY IN THIS CONVERSATION\n",
      "😊 Detected Emotion: disgust\n",
      "🤖 Gemini: It sounds like you're feeling a bit frustrated and unsure of how to continue the conversation. That's perfectly okay. Sometimes conversations just reach a natural end.  Is there anything else on your mind, or are you ready to finish our chat for now?  I'm happy to continue if you'd like, or I completely understand if you don't have anything further to discuss.\n",
      "\n",
      "🔴 Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Recording: 100%|██████████| 4/4 [00:04<00:00,  1.00s/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recording saved.\n",
      "🗣️ Transcript: \n",
      "😊 Detected Emotion: sad\n",
      "⚠️ No speech detected.\n",
      "\n",
      "👋 Exiting voice loop.\n"
     ]
    }
   ],
   "source": [
    "# === MAIN LOOP ===\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from tqdm import tqdm\n",
    "\n",
    "fs = 16000\n",
    "\n",
    "os.makedirs(\"recordings\", exist_ok=True)\n",
    "\n",
    "print(\"🎤 Voice interaction loop started. Press Ctrl+C to stop.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        clip_path = f\"recordings/clip_{timestamp}.wav\"\n",
    "\n",
    "        # Record audio\n",
    "        duration = 4  # seconds\n",
    "        print(\"🔴 Recording...\")\n",
    "        audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "\n",
    "        for _ in tqdm(range(duration), desc=\"⏳ Recording\", unit=\"s\"):\n",
    "            time.sleep(1)\n",
    "\n",
    "        sd.wait()\n",
    "        write(clip_path, 16000, audio)\n",
    "        print(\"✅ Recording saved.\")\n",
    "\n",
    "        # Transcribe and classify\n",
    "        transcript = transcribe_wav_clip(clip_path)\n",
    "        emotion = classify_emotion(clip_path)\n",
    "        print(f\"🗣️ Transcript: {transcript}\")\n",
    "        print(f\"😊 Detected Emotion: {emotion}\")\n",
    "\n",
    "        if transcript.strip():\n",
    "            reply = generate_reply_with_context(transcript, emotion)\n",
    "            print(f\"🤖 Gemini: {reply}\\n\")\n",
    "        else:\n",
    "            print(\"⚠️ No speech detected.\\n\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"👋 Exiting voice loop.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
